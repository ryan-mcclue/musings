# Workflow
Preparation Mentality:
Git usage

Terminal + Editor + Window manager

Programming Mentality:
it's not the programming practice but the dogma that gets you. when you start to name things it almost
always becomes bad. almost all programming practices have a place, just not used often
so RAII people, in case of things that must be released, e.g DeviceContext, ok to use a constructor/destructor
pair. I'll throw you a bone there

streaming i/o is almost never a good choice (hard drive slowest, more errors)

compression orientated programming is you code what you need at the time (breaking out into function, combining into struct, etc.) 
over time the code marches towards a better overall quality

amdahls law gives the time taken for execution given a number of cogives the time taken for execution given a number of cores.
for this formula (indeed any formula) we can obtain some property by seeing as function parameter approaches infinity. in this case, the parallelising part drops out.
brooks law says that simply adding more people to a problem does not necessarily make it faster. if requires great deal of coordination/communication actually slows down.

solving a problem: 1. decide what you are doing (this can't be open-ended.) 2. organise groups to achieve this
by making these boundaries, we are presupposing that each part is separate, e.g tyres team and engine team; assume tyres and engine cannot be one piece.
therefore, the boundaries define what products you can make, i.e. you produce products that are copies of yourself or how you are structured
so, in software if we assign teams for say audio, 2d, 3d we would expect individual APIs for each.
the org chart is the asymptote, so it's the best case that we make a product as granular as our org chart. it could be far worse and even more granular 
therefore, communication between teams is more costly than communication within teams.
takeaway is that low-cost things can be optimised, high-cost can't be (further away on the org chart)
note that communication in code could just be someone checking something in and you pulling it
what we are seeing now with modern software is the superposition of orgcharts due to use of legacy codebases
now we see org charts in software, where people are artifically creating inheritence hierarcies that limit how the program works
this is very bad. the reason it's done is for people to create mental models that help them solve the problem as they can't keep the complexity in their head. 
it may be necesary to solve the problem, however it shouldn't be looked at as good.
however, because it's done due to lack of understanding, the delegation/separation is not done with enough information. so you limit possibilities of the design space.
so although, libraries, microservices, encapsulation, package managers, engines may be necessary due to our brain capacity 
(until neuralink or we figure out a better way to do them) they are not good! 
we may use hash map, but only in a particular way
They limit optimisation as we have already decided separation
so always be on the lookout for times when you don't have to do these
most people just download hundreds of libraries because they know it works and they won't be worse than any one else.
WE MUST BE LEAN AND FLEXIBLE IN ORGCHARTS IN COMPANY AND IN SOFTWARE TO INCREASE DESIGN.
some old codebases need to be retired


DO THE 'MOST CERTAIN' THING FIRST. THIS COULD EITHER BE THE IMPLEMENTATION OR THE USAGE CODE
choose data structures around solving problem

some software is scaffolding, i.e. not shipped with the final product, e.g. editor for games

To make anything alternate over time, just multiply by sine(time);

Data hiding hides what the CPU is doing, which is what we care about

Require machine-specific documentation files to understand system we are on
System specific ctags template projects, e.g. linux kernel, glibc, etc.
If using library, have ctags for that project

ALMOST ALWAYS CAST TO FLOAT WHEN DOING DIVISIONS LEADING TO FLOAT

Minimum value starts at max

Spreading out randomness: `final_value += contrib * sample`

If debug code (or code that will not be in release) use compile-time macros

Use GLOBAL and global_prefix 
If casting is occuring, always be explicit about it!
Prefixing functions with sdl2_func() or linux_func() 

With error handling, bad practice is to allow a lot of errors, which brings in error classes etc.
Instead, if it's something that is actually an error, e.g. missing file, write the code to explicitly handle it.
Handling the error in a sense makes it no longer an error, rather a feature of the program

if function is expecting a range between, should we clamp to it?

Refactoring Mentality:
Don't be scared of mass name changing!! Before doing so, see all places where name is used
Don't be scared of long list of compiler errors. Work your way through them

Refactoring with usage code: just write out structures that satisfy the usage code.
If major rewrite use #if 0 #endif to allow for successful compiling

refactoring just copy code into function that gets it to compile.
later, worry about passing information in as a parameter
basic debug and release compiler flags
When refactoring, utilise our vimrc <C-F> all files
Also, just pull code out into desired function and let the compiler errors guide you
Returning multiple values, just return struct
To reduce large number of function parameters, put into struct


Debugging Mentality:
debugging stepping through pass-by-pass. 
inspecting all variables and parameters and verifying state of particular ones.
make deductions about state of variables, e.g overflowed, uninitialised, etc.
drop in asserts
draw it out

being able to draw out debug information is very useful. 
time spent visualising is never wasted (in debugger expressions also)

When debugging, look through variables and see if anything looks ridiculous

When in debugger, go iteratively progress through variable values in function and see if they look right
We can isolate some area of the code and say this is probably the problem
Then investigate relevent sub-functions, etc.
This can be a long process with seemingly little gains.
The issue could be subtle, e.g. signed/unsignedness size, function called rarely


Configuration files should be copied, not generated (becomes too messy)
Symlink to template files from projects

To begin, I ensured that I had a debugger 
from which I could easily step through the application's execution.
In code, I was able to programmatically set system and user breakpoints.

(mocking of syscalls for unit testing with file i/o)

For handling non-fatal errors, single line check. 
For fatal errors, nest all preceding code 
(I have learnt to not be afraid of indentation in this manner).
(error handling in general, i.e. reduce 'errors' by making them part of normal execution flow)

When performing the common task of grouping data, a few practices to keep in mind.
Use fixed sized types to always know about struct padding 
(in fact, I like to extend this to all my code)

If wanting multiple ways of accessing grouped data, use union and anonymous structs.
Use an int to reference other structs, e.g. `plane_index` 

If the data being grouped can only exist together (e.g. points), use vectors.
Put all structs related typedefs inside their own header file for easy access.

As floats are an approximation, when comparing to 0.0f (say for a denominator check) 
or negative (say for a square root) use a tolerance/epsilon less-than/greater-than check.
In fact whenever dividing should always ask oneself "can the value be zero?"
To be clear about float to int casting, use a macro like truncate/round (think about what if uneven divide)
Due to mixed integer and float arithmetic going to float, calculate integer percentages `val * 100 / total`
There is no need to overload the division operator as can do `(* 1.0f / val)`

For easy substitution, use single letter prefix names like `output_h` and `output_w`.
Convention for variable arrays, e.g. `Planes plane[1]`, `planes` and `plane_count (use ARRAY_COUNT macro here)` 
Put for loop statements on separate line to help not be afraid of indentation.
Iterate over pixel space and then convert to say, world space for calculations (normalisation and lerp)
Aspect ratio correction is simply rearranging a ratio. 
If we determine one side is larger, scale other.
Use "\r" ASCII code to print a status indicator.
Only use const for char * string literals stored in the data segment.

Endianness comes into play when reading/writing from disk (e.g. file type magic value) and working directly with `u8 *` (e.g. iterating through bytes of a u32) 
